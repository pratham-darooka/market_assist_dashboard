from bs4 import BeautifulSoup
import requests
from duckduckgo_search import DDGS

def get_mc_news():
    # Create dictionaries to store categorized headlines and URLs
    categorized_news = {
        'Stock Market News üìà': [],
        'Company News üßë‚Äçüíª': [],
        'All News üì∞': [],
    }

    # Define the URLs for each category
    urls = {
        'All News üì∞': [f'https://www.moneycontrol.com/news/news-all/page-{i}' for i in range(1, 3)],
        'Stock Market News üìà': ['https://www.moneycontrol.com/news/business/stocks'],
        'Company News üßë‚Äçüíª': ['https://www.moneycontrol.com/news/tags/companies.html']
    }

    # Function to process each URL and categorize news
    def process_url(url, category):
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')

        # Find all h2 tags with news headlines
        sections = soup.find_all('h2')

        for section in sections:
            a_tag = section.find('a')
            if a_tag:
                headline = a_tag.get_text(strip=True)
                link = a_tag['href']
                categorized_news[category].append({'headline': headline, 'url': link})

    # Process URLs for each category
    for category, url_list in urls.items():
        for url in url_list:
            process_url(url, category)

    return categorized_news

def get_et_news():
    # Create a dictionary to store categorized headlines and URLs
    categorized_news = {
        'Latest News üì∞': []
    }

    # Define the URL for the "just-in" section
    url = 'https://economictimes.indiatimes.com/markets'

    # Function to extract 'just-in' section
    def extract_latest_news(url):
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        just_in_section = soup.find('ul', id='just_in')

        if just_in_section:
            items = just_in_section.find_all('li')
            for item in items:
                a_tag = item.find('a')
                if a_tag:
                    headline = a_tag.get_text(strip=True)
                    link = a_tag['href']
                    categorized_news['Latest News üì∞'].append({'headline': headline, 'url': link})

    # Extract the latest news
    extract_latest_news(url)

    return categorized_news

def get_finshots_news():
    # Create a dictionary to store categorized headlines and URLs
    categorized_news = {
        'Daiy Finshots üì∞': []
    }

    # Define the URL for the "just-in" section
    url = 'https://economictimes.indiatimes.com/markets'

    # Function to extract 'just-in' section
    def extract_latest_news(url):
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        just_in_section = soup.find('ul', id='just_in')

        if just_in_section:
            items = just_in_section.find_all('li')
            for item in items:
                a_tag = item.find('a')
                if a_tag:
                    headline = a_tag.get_text(strip=True)
                    link = a_tag['href']
                    categorized_news['Latest News üì∞'].append({'headline': headline, 'url': link})

    # Extract the latest news
    extract_latest_news(url)

    return categorized_news

def get_finshots_news():
    # Create a dictionary to store categorized headlines and URLs
    categorized_news = {
        'Daily Articles üì∞': []
    }

    # Define the URL for the "archive" section
    url = 'https://finshots.in/archive/'

    # Function to extract the latest news
    def extract_latest_news(url):
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        # Find all articles in the post feed
        articles = soup.find_all('article', class_='post-card')

        for article in articles:
            # Find the title and the link
            link_tag = article.find('a', class_='post-card-content-link')
            title_tag = article.find('h2', class_='post-card-title')
            excerpt_tag = article.find('section', class_='post-card-excerpt')
            
            if link_tag and title_tag:
                headline = title_tag.get_text(strip=True)
                link = link_tag['href']
                excerpt = excerpt_tag.get_text(strip=True) if excerpt_tag else None
                categorized_news['Daily Articles üì∞'].append({'headline': headline, 'url': f'https://finshots.in{link}', 'excerpt': excerpt})

    # Extract the latest news
    extract_latest_news(url)

    return categorized_news

def format_news_as_markdown(categorized_news, include_excerpt=False, limit=5):
    markdown = ""
    for category, articles in categorized_news.items():
        markdown += f"#### {category}\n\n"
        for article in articles[:limit]:
            if include_excerpt and 'excerpt' in article:
                markdown += f"- **[{article['headline']}]({article['url']})**\n  \n  *{article['excerpt']}*\n"
            else:
                markdown += f"- [{article['headline']}]({article['url']})\n"
        markdown += "\n"
    return markdown

def format_ddg_news_as_markdown(categorized_news, include_excerpt=False, limit=5):
    markdown = ""
    for category, articles in categorized_news.items():
        markdown += f"#### {category}\n\n"
        for article in articles[:limit]:
            if include_excerpt and 'excerpt' in article:
                markdown += f"- **[{article['title']}]({article['url']})**\n  \n  *{article['excerpt']}*\n"
            else:
                markdown += f"- [{article['title']}]({article['url']})\n"
        markdown += "\n"
    return markdown

def fetch_recent_stock_news(name, symbol):
    time_limits = ["d", "w", "m"]
    time_labels = ["Last 24 hours' updates...", "Last week's updates...", "Last month's updates..."]

    for time_limit, time_label in zip(time_limits, time_labels):
        results = DDGS().news(keywords=f"'{name}' '{symbol}'", region="in-en", safesearch="off", timelimit=time_limit, max_results=10)
        if results:
            return {time_label: results}

    return {"Recent Updates": []}

